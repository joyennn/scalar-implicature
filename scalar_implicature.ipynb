{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAqzsJKsJau1Y8CZDISxi/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joyennn/scalar-implicature/blob/main/scalar_implicature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7dwazEjLrys",
        "outputId": "fbf593c5-aa56-4edc-96be-a1f8d15dcb68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Experiment1 ###\n",
        "\n",
        "#import data\n",
        "\n",
        "import csv\n",
        "\n",
        "def load_csv_to_list(file_path):\n",
        "    data = []\n",
        "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        for row in csv_reader:\n",
        "            data.append(row)\n",
        "    return data\n",
        "\n",
        "file_path = 'ex1_data.csv'\n",
        "data = load_csv_to_list(file_path)"
      ],
      "metadata": {
        "id": "YV_gCQezObHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#embeddings for BERT\n",
        "def bert_embeddings(sentences):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]   #CLS embeddings\n",
        "    return embeddings\n",
        "\n",
        "#embeddings for GPT-2\n",
        "def gpt2_embeddings(sentences):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = GPT2Model.from_pretrained('gpt2')\n",
        "    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)   #average of token embeddings\n",
        "    return embeddings\n",
        "\n",
        "# Cosine similarity and transformation\n",
        "def cosine_similarity(embeddings1, embeddings2):\n",
        "    cosine_sim = F.cosine_similarity(embeddings1, embeddings2).item()\n",
        "    linear_transformed = (cosine_sim + 1) / 2                      #linearly transformed\n",
        "    sigmoid_transformed = 1 / (1 + np.exp(-linear_transformed))    #sigmoid\n",
        "    return sigmoid_transformed"
      ],
      "metadata": {
        "id": "yF0LVg-BLuiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for i in range(len(data)):\n",
        "    sentence1 = data[i][0]\n",
        "    sentence2 = data[i][1]\n",
        "    interpretation = data[i][2]\n",
        "\n",
        "    bert_embeddings1 = bert_embeddings([sentence1])\n",
        "    bert_embeddings2 = bert_embeddings([sentence2])\n",
        "    gpt2_embeddings1 = gpt2_embeddings([sentence1])\n",
        "    gpt2_embeddings2 = gpt2_embeddings([sentence2])\n",
        "\n",
        "    bert_similarity = cosine_similarity(bert_embeddings1, bert_embeddings2)\n",
        "    gpt2_similarity = cosine_similarity(gpt2_embeddings1, gpt2_embeddings2)\n",
        "\n",
        "    result.append([sentence1, sentence2, interpretation, bert_similarity, gpt2_similarity])"
      ],
      "metadata": {
        "id": "MFt4ldbEL2xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save result to .csv\n",
        "\n",
        "def save_list_to_csv(data_list, file_path):\n",
        "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "        csv_writer = csv.writer(file)\n",
        "        for row in data_list:\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "file_path = 'ex1_result.csv'\n",
        "save_list_to_csv(result, file_path)"
      ],
      "metadata": {
        "id": "gjCHpZuiNRS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Experiment2 ###\n",
        "\n",
        "#import data\n",
        "\n",
        "import csv\n",
        "\n",
        "def load_csv_to_list(file_path):\n",
        "    data = []\n",
        "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        for row in csv_reader:\n",
        "            data.append(row)\n",
        "    return data\n",
        "\n",
        "file_path = 'ex2_data.csv'\n",
        "data = load_csv_to_list(file_path)"
      ],
      "metadata": {
        "id": "NG90HDAiVLmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction, GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "\n",
        "# probability for BERT\n",
        "def bert_nsp_probability(question, answer):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    inputs = tokenizer.encode_plus(question, answer, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    nsp_prob = probs[0, 0].item()\n",
        "    return nsp_prob\n",
        "\n",
        "# probability for GPT-2\n",
        "def gpt2_next_token_probability(question, answer):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    sequence = f\"{question} {answer}\"\n",
        "    inputs = tokenizer(sequence, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "    loss = outputs.loss.item()\n",
        "    next_token_prob = np.exp(-loss)\n",
        "    return next_token_prob\n",
        "\n",
        "# Surprisal\n",
        "def compute_surprisal(probability):\n",
        "    return -np.log2(probability)"
      ],
      "metadata": {
        "id": "HrY0uaciUU7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for i in range(len(data)):\n",
        "    question = data[i][0]\n",
        "    answer = data[i][1]\n",
        "    qud = data[i][2]\n",
        "\n",
        "    bert_probability = bert_nsp_probability(question, answer)\n",
        "    gpt2_probability = gpt2_next_token_probability(question, answer)\n",
        "\n",
        "    bert_surprisal = compute_surprisal(bert_probability)\n",
        "    gpt2_surprisal = compute_surprisal(gpt2_probability)\n",
        "\n",
        "    result.append([question, answer, qud, bert_surprisal, gpt2_surprisal])"
      ],
      "metadata": {
        "id": "TcRhjXviV7C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save result to .csv\n",
        "\n",
        "def save_list_to_csv(data_list, file_path):\n",
        "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "        csv_writer = csv.writer(file)\n",
        "        for row in data_list:\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "file_path = 'ex2_result.csv'\n",
        "save_list_to_csv(result, file_path)"
      ],
      "metadata": {
        "id": "sd65dxgnWrwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHypqIiQZ3OV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}